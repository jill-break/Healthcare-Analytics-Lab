Healthcare Analytics Lab: OLTP Performance Analysis
Dataset Size: ~10,000 - 40,000 rows (Test Subset)


QUESTION 1: Monthly Encounters by Specialty

SQL Query:
SELECT 
    DATE_FORMAT(e.encounter_date, '%Y-%m') AS encounter_month,
    s.specialty_name,
    e.encounter_type,
    COUNT(e.encounter_id) AS total_encounters,
    COUNT(DISTINCT e.patient_id) AS unique_patients
FROM 
    encounters e
JOIN 
    providers p ON e.provider_id = p.provider_id
JOIN 
    specialties s ON p.specialty_id = s.specialty_id
GROUP BY 
    DATE_FORMAT(e.encounter_date, '%Y-%m'),
    s.specialty_name,
    e.encounter_type
ORDER BY 
    encounter_month DESC, 
    s.specialty_name;

Schema Analysis:
- Tables joined: encounters, providers, specialties
- Number of joins: 2

Performance (Source: image_fed66e.png):
- Execution time: 0.046 seconds
- Estimated rows scanned: 40,000 (Rows Examined)
- Rows sent to client: 468
- Temporary Tables: 2 created

Bottleneck Identified:
1. Full Table Scan: The query examined 40,000 rows to return only 468. Even with a small dataset, it touched 4x more rows than existed in the limit, likely due to the join multiplication before grouping.
2. Temporary Tables: The engine had to create 2 temporary tables to handle the intermediate grouping results, adding overhead.
3. No Index Used: The "Index Usage" explicitly states "No Index used," forcing a brute-force scan.



QUESTION 2: Top Diagnosis-Procedure Pairs

SQL Query:
SELECT 
    d.icd10_code,
    p.cpt_code,
    COUNT(e.encounter_id) AS combo_count
FROM 
    encounters e
JOIN 
    encounter_diagnoses ed ON e.encounter_id = ed.encounter_id
JOIN 
    diagnoses d ON ed.diagnosis_id = d.diagnosis_id
JOIN 
    encounter_procedures ep ON e.encounter_id = ep.encounter_id
JOIN 
    procedures p ON ep.procedure_id = p.procedure_id
GROUP BY 
    d.icd10_code, 
    p.cpt_code
ORDER BY 
    combo_count DESC
LIMIT 10;

Schema Analysis:
- Tables joined: encounters, encounter_diagnoses, diagnoses, encounter_procedures, procedures
- Number of joins: 4

Performance (Source: image_fedaa9.png):
- Execution time: 0.094 seconds
- Estimated rows scanned: 49,820 (Rows Examined)
- Rows sent to client: 10
- Temporary Tables: 1 created

Bottleneck Identified:
1. Cartesian Explosion (Row Explosion): The "Rows Examined" (49,820) is 5x higher than the total number of encounters (10,000). This proves that joining two many-to-many tables (diagnoses and procedures) creates a multiplicative effect, forcing the database to process significantly more data than exists in the base table.
2. Inefficient Sorting: The database had to perform a "Sort with table scan" to group and order the results, requiring the creation of a temporary table.
3. High IO / Low Value: We processed nearly 50,000 rows just to show the user 10 lines of text.



QUESTION 3: 30-Day Readmission Rate

SQL Query:
SELECT 
    s.specialty_name,
    COUNT(DISTINCT e1.encounter_id) AS total_discharges,
    COUNT(DISTINCT e2.encounter_id) AS readmissions,
    (COUNT(DISTINCT e2.encounter_id) / COUNT(DISTINCT e1.encounter_id)) * 100 AS readmission_rate
FROM 
    encounters e1
JOIN 
    providers p ON e1.provider_id = p.provider_id
JOIN 
    specialties s ON p.specialty_id = s.specialty_id
LEFT JOIN 
    encounters e2 ON e1.patient_id = e2.patient_id 
    AND e2.encounter_date > e1.discharge_date 
    AND e2.encounter_date <= DATE_ADD(e1.discharge_date, INTERVAL 30 DAY)
WHERE 
    e1.encounter_type = 'Inpatient'
GROUP BY 
    s.specialty_name
ORDER BY 
    readmission_rate DESC;

Schema Analysis:
- Tables joined: encounters (Self-Joined as e1 and e2), providers, specialties
- Number of joins: 3 (Complex self-join involved)

Performance (Source: image_fedb28.png):
- Execution time: 0.047 seconds
- Estimated rows scanned: 26,869 (Rows Examined)
- Rows sent to client: 12
- Temporary Tables: 8 created (!!)

Bottleneck Identified:
1. Temporary Table Explosion: The statistics show "Temporary tables created: 8". This is excessively high. It indicates that the database engine could not resolve the complex self-join and aggregation steps in a single pass or in memory, forcing it to repeatedly write intermediate results to temporary storage.
2. Inefficient Range Scan: The condition 'e2.encounter_date <= DATE_ADD(...)' prevents the use of a simple direct lookup (hash join). For every inpatient encounter, the database must scan a "window" of time in the table, resulting in 26,869 row examinations.
3. No Index Usage: The "Index Usage" is marked as "No Index used," confirming that the query is performing full table scans.



QUESTION 4: Revenue by Specialty & Month

SQL Query:
SELECT 
    s.specialty_name,
    DATE_FORMAT(b.claim_date, '%Y-%m') AS billing_month,
    SUM(b.allowed_amount) AS total_revenue
FROM 
    billing b
JOIN 
    encounters e ON b.encounter_id = e.encounter_id
JOIN 
    providers p ON e.provider_id = p.provider_id
JOIN 
    specialties s ON p.specialty_id = s.specialty_id
GROUP BY 
    s.specialty_name,
    DATE_FORMAT(b.claim_date, '%Y-%m')
ORDER BY 
    total_revenue DESC;

Schema Analysis:
- Tables joined: billing, encounters, providers, specialties
- Number of joins: 3

Performance (Source: image_ff2cff.png):
- Execution time: 0.125 seconds
- Estimated rows scanned: 40,012 (Rows Examined)
- Rows sent to client: 12
- Temporary Tables: 1 created

Bottleneck Identified:
1. Long Join Chain: To aggregate revenue by specialty, the database must join across 4 tables (Billing -> Encounters -> Providers -> Specialties).
2. Full Scan Overhead: "Rows Examined" is 40,012. The engine examined 4x the number of base rows (10,000) due to the join overhead before aggregation could occur.
3. Slowest Query in Batch: At 0.125s, this was the slowest query in the batch (excluding the Q2 outliers), confirming that joining large transactional tables (Billing + Encounters) is computationally expensive.